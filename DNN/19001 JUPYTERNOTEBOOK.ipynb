{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSFW_DNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7aO01i2VnPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7942c3aa-0898-4e75-dcd0-e03ee0dfb7ee"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install jovian opendatasets --upgrade --quiet\n",
        "import opendatasets as od"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.9 MB/s \n",
            "\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8W0E_XeWCqM"
      },
      "source": [
        "dataset_url = 'https://www.kaggle.com/omeret/not-safe-for-work'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXmtbyCdWJGu",
        "outputId": "1311677c-b0a5-40aa-f46e-01297abecc12"
      },
      "source": [
        "!pip install jovian opendatasets --upgrade --quiet\n",
        "import opendatasets as od\n",
        "od.download(dataset_url)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Your Kaggle Key: Downloading not-safe-for-work.zip to ./not-safe-for-work\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19.4G/19.4G [06:34<00:00, 52.9MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf35qnUKWSM_"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEy-QdlGa52V",
        "outputId": "509333e1-1d2d-491a-fc0c-bc122ae470a1"
      },
      "source": [
        "data_dir = './not-safe-for-work'\n",
        "\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train']\n",
            "['nsfw', 'sfw']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz3oWFEqbC9f"
      },
      "source": [
        "mean = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomRotation(degrees=40),\n",
        "\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqsvBWZkbGNH"
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpBxBPtgbIgv"
      },
      "source": [
        "trainset = ImageFolder(data_dir+'/train', transform=transform)\n",
        "testset = ImageFolder(data_dir+'/test', transform=transform)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFTKVHewbU3H",
        "outputId": "d27c491d-f21f-41df-933f-98bac28aa26b"
      },
      "source": [
        "trainset"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 98531\n",
              "    Root location: ./not-safe-for-work/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomHorizontalFlip(p=0.3)\n",
              "               RandomRotation(degrees=[-40.0, 40.0], interpolation=nearest, expand=False, fill=0)\n",
              "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               CenterCrop(size=(256, 256))\n",
              "               ToTensor()\n",
              "               Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4uRXMLbeCv"
      },
      "source": [
        "trainset_subset = torch.utils.data.Subset(trainset, np.random.choice(len(trainset), 10000, replace=False))\n",
        "testset_subset = torch.utils.data.Subset(testset, np.random.choice(len(testset), 1000, replace=False))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep88V5sYboY-",
        "outputId": "fc1dadf2-f24a-492f-e9a0-e48eccdc9df6"
      },
      "source": [
        "img, label = trainset_subset[0]\n",
        "print(img.shape, label)\n",
        "img"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 256, 256]) 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.2929,  1.3443,  1.4470,  ..., -2.1834, -2.1834, -2.1834],\n",
              "         [ 1.2929,  1.3100,  1.4470,  ..., -2.1834, -2.1834, -2.1834],\n",
              "         [ 1.3957,  1.3443,  1.4128,  ..., -2.1834, -2.1834, -2.1834],\n",
              "         ...,\n",
              "         [-2.1834, -2.1834, -2.1834,  ...,  0.7107,  0.7278,  0.5566],\n",
              "         [-2.1834, -2.1834, -2.1834,  ...,  0.3168,  0.6593,  1.0532],\n",
              "         [-2.1834, -2.1834, -2.1834,  ...,  1.0703,  0.4024,  0.4024]],\n",
              "\n",
              "        [[ 1.1642,  1.2167,  1.3393,  ..., -2.2321, -2.2321, -2.2321],\n",
              "         [ 1.1817,  1.1992,  1.3393,  ..., -2.2321, -2.2321, -2.2321],\n",
              "         [ 1.2868,  1.2342,  1.3043,  ..., -2.2321, -2.2321, -2.2321],\n",
              "         ...,\n",
              "         [-2.2321, -2.2321, -2.2321,  ...,  0.5690,  0.5865,  0.4114],\n",
              "         [-2.2321, -2.2321, -2.2321,  ...,  0.1663,  0.5165,  0.9191],\n",
              "         [-2.2321, -2.2321, -2.2321,  ...,  0.9366,  0.2539,  0.2539]],\n",
              "\n",
              "        [[ 1.0370,  1.0719,  1.1416,  ..., -2.2222, -2.2222, -2.2222],\n",
              "         [ 1.0022,  1.0196,  1.1416,  ..., -2.2222, -2.2222, -2.2222],\n",
              "         [ 1.0893,  1.0370,  1.1068,  ..., -2.2222, -2.2222, -2.2222],\n",
              "         ...,\n",
              "         [-2.2222, -2.2222, -2.2222,  ...,  0.4793,  0.4967,  0.3224],\n",
              "         [-2.2222, -2.2222, -2.2222,  ...,  0.0784,  0.4270,  0.8279],\n",
              "         [-2.2222, -2.2222, -2.2222,  ...,  0.8453,  0.1656,  0.1656]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2pY1FMJbwE2"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=trainset_subset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbA3hKNpdg0Q"
      },
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=testset_subset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjzoZ62Bg_-x"
      },
      "source": [
        "Hidden Layers, Activation Functions and Non-Linearity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXMq2Hg7XCZq"
      },
      "source": [
        "input_size = 65536*3\n",
        "hidden_size = 500 \n",
        "num_classes = 2\n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001 "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOPgZ6WJ8VF8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcOnEPSJhCHU"
      },
      "source": [
        "import torch.nn as nn\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.l1 = nn.Linear(input_size, hidden_size) \n",
        "        self.TanH = nn.Tanh()\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.TanH(out)\n",
        "        out = self.l2(out)\n",
        "        # no activation and no softmax at the end\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZnfWxp3k1Do"
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVjhT8wLXRhO",
        "outputId": "5fb9e61a-588c-4f72-ee6d-60dad1160992"
      },
      "source": [
        "n_total_steps = len(train_dataloader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_dataloader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 256*256*3).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "          print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/625], Loss: 0.2664\n",
            "Epoch [1/10], Step [20/625], Loss: 0.3763\n",
            "Epoch [1/10], Step [30/625], Loss: 0.5690\n",
            "Epoch [1/10], Step [40/625], Loss: 0.8374\n",
            "Epoch [1/10], Step [50/625], Loss: 0.5718\n",
            "Epoch [1/10], Step [60/625], Loss: 0.8701\n",
            "Epoch [1/10], Step [70/625], Loss: 0.5113\n",
            "Epoch [1/10], Step [80/625], Loss: 0.3658\n",
            "Epoch [1/10], Step [90/625], Loss: 0.4565\n",
            "Epoch [1/10], Step [100/625], Loss: 0.4613\n",
            "Epoch [1/10], Step [110/625], Loss: 0.5829\n",
            "Epoch [1/10], Step [120/625], Loss: 0.6000\n",
            "Epoch [1/10], Step [130/625], Loss: 0.5048\n",
            "Epoch [1/10], Step [140/625], Loss: 0.7548\n",
            "Epoch [1/10], Step [150/625], Loss: 0.4604\n",
            "Epoch [1/10], Step [160/625], Loss: 0.5904\n",
            "Epoch [1/10], Step [170/625], Loss: 0.3119\n",
            "Epoch [1/10], Step [180/625], Loss: 0.7902\n",
            "Epoch [1/10], Step [190/625], Loss: 0.7821\n",
            "Epoch [1/10], Step [200/625], Loss: 0.4121\n",
            "Epoch [1/10], Step [210/625], Loss: 0.4916\n",
            "Epoch [1/10], Step [220/625], Loss: 0.4490\n",
            "Epoch [1/10], Step [230/625], Loss: 0.5961\n",
            "Epoch [1/10], Step [240/625], Loss: 0.5531\n",
            "Epoch [1/10], Step [250/625], Loss: 0.7924\n",
            "Epoch [1/10], Step [260/625], Loss: 0.6162\n",
            "Epoch [1/10], Step [270/625], Loss: 0.6356\n",
            "Epoch [1/10], Step [280/625], Loss: 0.3861\n",
            "Epoch [1/10], Step [290/625], Loss: 0.6850\n",
            "Epoch [1/10], Step [300/625], Loss: 0.5157\n",
            "Epoch [1/10], Step [310/625], Loss: 0.4929\n",
            "Epoch [1/10], Step [320/625], Loss: 0.4375\n",
            "Epoch [1/10], Step [330/625], Loss: 0.6457\n",
            "Epoch [1/10], Step [340/625], Loss: 0.5393\n",
            "Epoch [1/10], Step [350/625], Loss: 0.3071\n",
            "Epoch [1/10], Step [360/625], Loss: 0.3811\n",
            "Epoch [1/10], Step [370/625], Loss: 0.7522\n",
            "Epoch [1/10], Step [380/625], Loss: 0.5885\n",
            "Epoch [1/10], Step [390/625], Loss: 0.5721\n",
            "Epoch [1/10], Step [400/625], Loss: 0.4428\n",
            "Epoch [1/10], Step [410/625], Loss: 0.5853\n",
            "Epoch [1/10], Step [420/625], Loss: 0.6641\n",
            "Epoch [1/10], Step [430/625], Loss: 0.7195\n",
            "Epoch [1/10], Step [440/625], Loss: 0.7045\n",
            "Epoch [1/10], Step [450/625], Loss: 0.5343\n",
            "Epoch [1/10], Step [460/625], Loss: 0.6258\n",
            "Epoch [1/10], Step [470/625], Loss: 0.3059\n",
            "Epoch [1/10], Step [480/625], Loss: 0.4103\n",
            "Epoch [1/10], Step [490/625], Loss: 0.4264\n",
            "Epoch [1/10], Step [500/625], Loss: 0.4103\n",
            "Epoch [1/10], Step [510/625], Loss: 0.4716\n",
            "Epoch [1/10], Step [520/625], Loss: 0.8273\n",
            "Epoch [1/10], Step [530/625], Loss: 0.4018\n",
            "Epoch [1/10], Step [540/625], Loss: 0.6541\n",
            "Epoch [1/10], Step [550/625], Loss: 0.4541\n",
            "Epoch [1/10], Step [560/625], Loss: 0.4030\n",
            "Epoch [1/10], Step [570/625], Loss: 0.7795\n",
            "Epoch [1/10], Step [580/625], Loss: 0.3785\n",
            "Epoch [1/10], Step [590/625], Loss: 0.5201\n",
            "Epoch [1/10], Step [600/625], Loss: 0.2665\n",
            "Epoch [1/10], Step [610/625], Loss: 0.3778\n",
            "Epoch [1/10], Step [620/625], Loss: 0.4335\n",
            "Epoch [2/10], Step [10/625], Loss: 0.3024\n",
            "Epoch [2/10], Step [20/625], Loss: 0.3419\n",
            "Epoch [2/10], Step [30/625], Loss: 0.6359\n",
            "Epoch [2/10], Step [40/625], Loss: 0.6207\n",
            "Epoch [2/10], Step [50/625], Loss: 0.4410\n",
            "Epoch [2/10], Step [60/625], Loss: 0.6223\n",
            "Epoch [2/10], Step [70/625], Loss: 0.5582\n",
            "Epoch [2/10], Step [80/625], Loss: 0.4648\n",
            "Epoch [2/10], Step [90/625], Loss: 0.3956\n",
            "Epoch [2/10], Step [100/625], Loss: 0.4316\n",
            "Epoch [2/10], Step [110/625], Loss: 0.6236\n",
            "Epoch [2/10], Step [120/625], Loss: 0.5100\n",
            "Epoch [2/10], Step [130/625], Loss: 0.5566\n",
            "Epoch [2/10], Step [140/625], Loss: 0.8554\n",
            "Epoch [2/10], Step [150/625], Loss: 0.4988\n",
            "Epoch [2/10], Step [160/625], Loss: 0.9603\n",
            "Epoch [2/10], Step [170/625], Loss: 0.2875\n",
            "Epoch [2/10], Step [180/625], Loss: 0.7388\n",
            "Epoch [2/10], Step [190/625], Loss: 0.7970\n",
            "Epoch [2/10], Step [200/625], Loss: 0.4417\n",
            "Epoch [2/10], Step [210/625], Loss: 0.5469\n",
            "Epoch [2/10], Step [220/625], Loss: 0.5783\n",
            "Epoch [2/10], Step [230/625], Loss: 0.6730\n",
            "Epoch [2/10], Step [240/625], Loss: 0.4613\n",
            "Epoch [2/10], Step [250/625], Loss: 0.5708\n",
            "Epoch [2/10], Step [260/625], Loss: 0.6529\n",
            "Epoch [2/10], Step [270/625], Loss: 0.6569\n",
            "Epoch [2/10], Step [280/625], Loss: 0.6382\n",
            "Epoch [2/10], Step [290/625], Loss: 0.7376\n",
            "Epoch [2/10], Step [300/625], Loss: 0.5962\n",
            "Epoch [2/10], Step [310/625], Loss: 0.6512\n",
            "Epoch [2/10], Step [320/625], Loss: 0.3764\n",
            "Epoch [2/10], Step [330/625], Loss: 0.8124\n",
            "Epoch [2/10], Step [340/625], Loss: 0.5299\n",
            "Epoch [2/10], Step [350/625], Loss: 0.2468\n",
            "Epoch [2/10], Step [360/625], Loss: 0.3180\n",
            "Epoch [2/10], Step [370/625], Loss: 0.6479\n",
            "Epoch [2/10], Step [380/625], Loss: 0.6884\n",
            "Epoch [2/10], Step [390/625], Loss: 0.6014\n",
            "Epoch [2/10], Step [400/625], Loss: 0.5010\n",
            "Epoch [2/10], Step [410/625], Loss: 0.4426\n",
            "Epoch [2/10], Step [420/625], Loss: 0.5484\n",
            "Epoch [2/10], Step [430/625], Loss: 0.8402\n",
            "Epoch [2/10], Step [440/625], Loss: 0.9176\n",
            "Epoch [2/10], Step [450/625], Loss: 0.4025\n",
            "Epoch [2/10], Step [460/625], Loss: 0.6896\n",
            "Epoch [2/10], Step [470/625], Loss: 0.2513\n",
            "Epoch [2/10], Step [480/625], Loss: 0.4695\n",
            "Epoch [2/10], Step [490/625], Loss: 0.4956\n",
            "Epoch [2/10], Step [500/625], Loss: 0.4915\n",
            "Epoch [2/10], Step [510/625], Loss: 0.4451\n",
            "Epoch [2/10], Step [520/625], Loss: 0.6479\n",
            "Epoch [2/10], Step [530/625], Loss: 0.3365\n",
            "Epoch [2/10], Step [540/625], Loss: 0.5041\n",
            "Epoch [2/10], Step [550/625], Loss: 0.4886\n",
            "Epoch [2/10], Step [560/625], Loss: 0.3959\n",
            "Epoch [2/10], Step [570/625], Loss: 0.6008\n",
            "Epoch [2/10], Step [580/625], Loss: 0.3833\n",
            "Epoch [2/10], Step [590/625], Loss: 0.3688\n",
            "Epoch [2/10], Step [600/625], Loss: 0.2908\n",
            "Epoch [2/10], Step [610/625], Loss: 0.3362\n",
            "Epoch [2/10], Step [620/625], Loss: 0.4963\n",
            "Epoch [3/10], Step [10/625], Loss: 0.3182\n",
            "Epoch [3/10], Step [20/625], Loss: 0.4409\n",
            "Epoch [3/10], Step [30/625], Loss: 0.4932\n",
            "Epoch [3/10], Step [40/625], Loss: 0.6484\n",
            "Epoch [3/10], Step [50/625], Loss: 0.4339\n",
            "Epoch [3/10], Step [60/625], Loss: 0.6605\n",
            "Epoch [3/10], Step [70/625], Loss: 0.5125\n",
            "Epoch [3/10], Step [80/625], Loss: 0.4644\n",
            "Epoch [3/10], Step [90/625], Loss: 0.4456\n",
            "Epoch [3/10], Step [100/625], Loss: 0.4704\n",
            "Epoch [3/10], Step [110/625], Loss: 0.5776\n",
            "Epoch [3/10], Step [120/625], Loss: 0.5150\n",
            "Epoch [3/10], Step [130/625], Loss: 0.5096\n",
            "Epoch [3/10], Step [140/625], Loss: 0.9020\n",
            "Epoch [3/10], Step [150/625], Loss: 0.4551\n",
            "Epoch [3/10], Step [160/625], Loss: 0.6706\n",
            "Epoch [3/10], Step [170/625], Loss: 0.4168\n",
            "Epoch [3/10], Step [180/625], Loss: 0.5807\n",
            "Epoch [3/10], Step [190/625], Loss: 0.6559\n",
            "Epoch [3/10], Step [200/625], Loss: 0.5149\n",
            "Epoch [3/10], Step [210/625], Loss: 0.4861\n",
            "Epoch [3/10], Step [220/625], Loss: 0.5714\n",
            "Epoch [3/10], Step [230/625], Loss: 0.6714\n",
            "Epoch [3/10], Step [240/625], Loss: 0.5703\n",
            "Epoch [3/10], Step [250/625], Loss: 0.6552\n",
            "Epoch [3/10], Step [260/625], Loss: 0.6446\n",
            "Epoch [3/10], Step [270/625], Loss: 0.7631\n",
            "Epoch [3/10], Step [280/625], Loss: 0.5951\n",
            "Epoch [3/10], Step [290/625], Loss: 0.6155\n",
            "Epoch [3/10], Step [300/625], Loss: 0.5870\n",
            "Epoch [3/10], Step [310/625], Loss: 0.5148\n",
            "Epoch [3/10], Step [320/625], Loss: 0.5466\n",
            "Epoch [3/10], Step [330/625], Loss: 0.6684\n",
            "Epoch [3/10], Step [340/625], Loss: 0.5770\n",
            "Epoch [3/10], Step [350/625], Loss: 0.2738\n",
            "Epoch [3/10], Step [360/625], Loss: 0.3173\n",
            "Epoch [3/10], Step [370/625], Loss: 0.6765\n",
            "Epoch [3/10], Step [380/625], Loss: 0.5676\n",
            "Epoch [3/10], Step [390/625], Loss: 0.5123\n",
            "Epoch [3/10], Step [400/625], Loss: 0.4349\n",
            "Epoch [3/10], Step [410/625], Loss: 0.4341\n",
            "Epoch [3/10], Step [420/625], Loss: 0.5483\n",
            "Epoch [3/10], Step [430/625], Loss: 0.7693\n",
            "Epoch [3/10], Step [440/625], Loss: 0.6738\n",
            "Epoch [3/10], Step [450/625], Loss: 0.4283\n",
            "Epoch [3/10], Step [460/625], Loss: 0.5679\n",
            "Epoch [3/10], Step [470/625], Loss: 0.3040\n",
            "Epoch [3/10], Step [480/625], Loss: 0.4169\n",
            "Epoch [3/10], Step [490/625], Loss: 0.5261\n",
            "Epoch [3/10], Step [500/625], Loss: 0.5337\n",
            "Epoch [3/10], Step [510/625], Loss: 0.4150\n",
            "Epoch [3/10], Step [520/625], Loss: 0.7318\n",
            "Epoch [3/10], Step [530/625], Loss: 0.3437\n",
            "Epoch [3/10], Step [540/625], Loss: 0.6237\n",
            "Epoch [3/10], Step [550/625], Loss: 0.3960\n",
            "Epoch [3/10], Step [560/625], Loss: 0.4314\n",
            "Epoch [3/10], Step [570/625], Loss: 0.6666\n",
            "Epoch [3/10], Step [580/625], Loss: 0.3740\n",
            "Epoch [3/10], Step [590/625], Loss: 0.4684\n",
            "Epoch [3/10], Step [600/625], Loss: 0.3139\n",
            "Epoch [3/10], Step [610/625], Loss: 0.3323\n",
            "Epoch [3/10], Step [620/625], Loss: 0.4349\n",
            "Epoch [4/10], Step [10/625], Loss: 0.2456\n",
            "Epoch [4/10], Step [20/625], Loss: 0.3737\n",
            "Epoch [4/10], Step [30/625], Loss: 0.6229\n",
            "Epoch [4/10], Step [40/625], Loss: 0.6509\n",
            "Epoch [4/10], Step [50/625], Loss: 0.4193\n",
            "Epoch [4/10], Step [60/625], Loss: 0.8396\n",
            "Epoch [4/10], Step [70/625], Loss: 0.5161\n",
            "Epoch [4/10], Step [80/625], Loss: 0.4166\n",
            "Epoch [4/10], Step [90/625], Loss: 0.4233\n",
            "Epoch [4/10], Step [100/625], Loss: 0.4545\n",
            "Epoch [4/10], Step [110/625], Loss: 0.5368\n",
            "Epoch [4/10], Step [120/625], Loss: 0.5543\n",
            "Epoch [4/10], Step [130/625], Loss: 0.6096\n",
            "Epoch [4/10], Step [140/625], Loss: 0.8463\n",
            "Epoch [4/10], Step [150/625], Loss: 0.6357\n",
            "Epoch [4/10], Step [160/625], Loss: 0.8044\n",
            "Epoch [4/10], Step [170/625], Loss: 0.4053\n",
            "Epoch [4/10], Step [180/625], Loss: 0.6728\n",
            "Epoch [4/10], Step [190/625], Loss: 0.8076\n",
            "Epoch [4/10], Step [200/625], Loss: 0.5560\n",
            "Epoch [4/10], Step [210/625], Loss: 0.5851\n",
            "Epoch [4/10], Step [220/625], Loss: 0.4759\n",
            "Epoch [4/10], Step [230/625], Loss: 0.5716\n",
            "Epoch [4/10], Step [240/625], Loss: 0.5043\n",
            "Epoch [4/10], Step [250/625], Loss: 0.7201\n",
            "Epoch [4/10], Step [260/625], Loss: 0.5260\n",
            "Epoch [4/10], Step [270/625], Loss: 0.6429\n",
            "Epoch [4/10], Step [280/625], Loss: 0.5133\n",
            "Epoch [4/10], Step [290/625], Loss: 0.7472\n",
            "Epoch [4/10], Step [300/625], Loss: 0.5633\n",
            "Epoch [4/10], Step [310/625], Loss: 0.5002\n",
            "Epoch [4/10], Step [320/625], Loss: 0.4733\n",
            "Epoch [4/10], Step [330/625], Loss: 0.6863\n",
            "Epoch [4/10], Step [340/625], Loss: 0.7299\n",
            "Epoch [4/10], Step [350/625], Loss: 0.3308\n",
            "Epoch [4/10], Step [360/625], Loss: 0.2788\n",
            "Epoch [4/10], Step [370/625], Loss: 0.7601\n",
            "Epoch [4/10], Step [380/625], Loss: 0.7407\n",
            "Epoch [4/10], Step [390/625], Loss: 0.5204\n",
            "Epoch [4/10], Step [400/625], Loss: 0.4687\n",
            "Epoch [4/10], Step [410/625], Loss: 0.4136\n",
            "Epoch [4/10], Step [420/625], Loss: 0.5758\n",
            "Epoch [4/10], Step [430/625], Loss: 0.6202\n",
            "Epoch [4/10], Step [440/625], Loss: 0.7531\n",
            "Epoch [4/10], Step [450/625], Loss: 0.4381\n",
            "Epoch [4/10], Step [460/625], Loss: 0.5348\n",
            "Epoch [4/10], Step [470/625], Loss: 0.3589\n",
            "Epoch [4/10], Step [480/625], Loss: 0.5200\n",
            "Epoch [4/10], Step [490/625], Loss: 0.5410\n",
            "Epoch [4/10], Step [500/625], Loss: 0.4952\n",
            "Epoch [4/10], Step [510/625], Loss: 0.3804\n",
            "Epoch [4/10], Step [520/625], Loss: 0.6951\n",
            "Epoch [4/10], Step [530/625], Loss: 0.4162\n",
            "Epoch [4/10], Step [540/625], Loss: 0.7131\n",
            "Epoch [4/10], Step [550/625], Loss: 0.3740\n",
            "Epoch [4/10], Step [560/625], Loss: 0.4608\n",
            "Epoch [4/10], Step [570/625], Loss: 0.7829\n",
            "Epoch [4/10], Step [580/625], Loss: 0.3846\n",
            "Epoch [4/10], Step [590/625], Loss: 0.3888\n",
            "Epoch [4/10], Step [600/625], Loss: 0.2207\n",
            "Epoch [4/10], Step [610/625], Loss: 0.3318\n",
            "Epoch [4/10], Step [620/625], Loss: 0.3927\n",
            "Epoch [5/10], Step [10/625], Loss: 0.2501\n",
            "Epoch [5/10], Step [20/625], Loss: 0.3500\n",
            "Epoch [5/10], Step [30/625], Loss: 0.4806\n",
            "Epoch [5/10], Step [40/625], Loss: 0.5916\n",
            "Epoch [5/10], Step [50/625], Loss: 0.3651\n",
            "Epoch [5/10], Step [60/625], Loss: 0.6389\n",
            "Epoch [5/10], Step [70/625], Loss: 0.6563\n",
            "Epoch [5/10], Step [80/625], Loss: 0.4298\n",
            "Epoch [5/10], Step [90/625], Loss: 0.4365\n",
            "Epoch [5/10], Step [100/625], Loss: 0.5000\n",
            "Epoch [5/10], Step [110/625], Loss: 0.4632\n",
            "Epoch [5/10], Step [120/625], Loss: 0.6483\n",
            "Epoch [5/10], Step [130/625], Loss: 0.6758\n",
            "Epoch [5/10], Step [140/625], Loss: 0.9479\n",
            "Epoch [5/10], Step [150/625], Loss: 0.5330\n",
            "Epoch [5/10], Step [160/625], Loss: 0.7274\n",
            "Epoch [5/10], Step [170/625], Loss: 0.3465\n",
            "Epoch [5/10], Step [180/625], Loss: 0.7214\n",
            "Epoch [5/10], Step [190/625], Loss: 0.7064\n",
            "Epoch [5/10], Step [200/625], Loss: 0.4566\n",
            "Epoch [5/10], Step [210/625], Loss: 0.5169\n",
            "Epoch [5/10], Step [220/625], Loss: 0.5565\n",
            "Epoch [5/10], Step [230/625], Loss: 0.7902\n",
            "Epoch [5/10], Step [240/625], Loss: 0.5629\n",
            "Epoch [5/10], Step [250/625], Loss: 0.6913\n",
            "Epoch [5/10], Step [260/625], Loss: 0.5668\n",
            "Epoch [5/10], Step [270/625], Loss: 0.7927\n",
            "Epoch [5/10], Step [280/625], Loss: 0.5503\n",
            "Epoch [5/10], Step [290/625], Loss: 0.6176\n",
            "Epoch [5/10], Step [300/625], Loss: 0.4497\n",
            "Epoch [5/10], Step [310/625], Loss: 0.5043\n",
            "Epoch [5/10], Step [320/625], Loss: 0.3788\n",
            "Epoch [5/10], Step [330/625], Loss: 0.6766\n",
            "Epoch [5/10], Step [340/625], Loss: 0.5479\n",
            "Epoch [5/10], Step [350/625], Loss: 0.2899\n",
            "Epoch [5/10], Step [360/625], Loss: 0.3419\n",
            "Epoch [5/10], Step [370/625], Loss: 0.8940\n",
            "Epoch [5/10], Step [380/625], Loss: 0.6042\n",
            "Epoch [5/10], Step [390/625], Loss: 0.4563\n",
            "Epoch [5/10], Step [400/625], Loss: 0.5447\n",
            "Epoch [5/10], Step [410/625], Loss: 0.4278\n",
            "Epoch [5/10], Step [420/625], Loss: 0.6270\n",
            "Epoch [5/10], Step [430/625], Loss: 0.7746\n",
            "Epoch [5/10], Step [440/625], Loss: 0.7943\n",
            "Epoch [5/10], Step [450/625], Loss: 0.4362\n",
            "Epoch [5/10], Step [460/625], Loss: 0.5560\n",
            "Epoch [5/10], Step [470/625], Loss: 0.3018\n",
            "Epoch [5/10], Step [480/625], Loss: 0.5307\n",
            "Epoch [5/10], Step [490/625], Loss: 0.4312\n",
            "Epoch [5/10], Step [500/625], Loss: 0.3820\n",
            "Epoch [5/10], Step [510/625], Loss: 0.4085\n",
            "Epoch [5/10], Step [520/625], Loss: 0.5762\n",
            "Epoch [5/10], Step [530/625], Loss: 0.3601\n",
            "Epoch [5/10], Step [540/625], Loss: 0.8469\n",
            "Epoch [5/10], Step [550/625], Loss: 0.5058\n",
            "Epoch [5/10], Step [560/625], Loss: 0.3668\n",
            "Epoch [5/10], Step [570/625], Loss: 0.7180\n",
            "Epoch [5/10], Step [580/625], Loss: 0.4992\n",
            "Epoch [5/10], Step [590/625], Loss: 0.3598\n",
            "Epoch [5/10], Step [600/625], Loss: 0.2264\n",
            "Epoch [5/10], Step [610/625], Loss: 0.2927\n",
            "Epoch [5/10], Step [620/625], Loss: 0.4061\n",
            "Epoch [6/10], Step [10/625], Loss: 0.2877\n",
            "Epoch [6/10], Step [20/625], Loss: 0.2843\n",
            "Epoch [6/10], Step [30/625], Loss: 0.4607\n",
            "Epoch [6/10], Step [40/625], Loss: 0.7859\n",
            "Epoch [6/10], Step [50/625], Loss: 0.3423\n",
            "Epoch [6/10], Step [60/625], Loss: 0.6813\n",
            "Epoch [6/10], Step [70/625], Loss: 0.4386\n",
            "Epoch [6/10], Step [80/625], Loss: 0.5237\n",
            "Epoch [6/10], Step [90/625], Loss: 0.3985\n",
            "Epoch [6/10], Step [100/625], Loss: 0.4422\n",
            "Epoch [6/10], Step [110/625], Loss: 0.6149\n",
            "Epoch [6/10], Step [120/625], Loss: 0.6110\n",
            "Epoch [6/10], Step [130/625], Loss: 0.5476\n",
            "Epoch [6/10], Step [140/625], Loss: 0.6510\n",
            "Epoch [6/10], Step [150/625], Loss: 0.5923\n",
            "Epoch [6/10], Step [160/625], Loss: 0.8592\n",
            "Epoch [6/10], Step [170/625], Loss: 0.3276\n",
            "Epoch [6/10], Step [180/625], Loss: 0.8095\n",
            "Epoch [6/10], Step [190/625], Loss: 0.8036\n",
            "Epoch [6/10], Step [200/625], Loss: 0.5063\n",
            "Epoch [6/10], Step [210/625], Loss: 0.6710\n",
            "Epoch [6/10], Step [220/625], Loss: 0.4949\n",
            "Epoch [6/10], Step [230/625], Loss: 0.6471\n",
            "Epoch [6/10], Step [240/625], Loss: 0.5333\n",
            "Epoch [6/10], Step [250/625], Loss: 0.7045\n",
            "Epoch [6/10], Step [260/625], Loss: 0.5451\n",
            "Epoch [6/10], Step [270/625], Loss: 0.6873\n",
            "Epoch [6/10], Step [280/625], Loss: 0.4770\n",
            "Epoch [6/10], Step [290/625], Loss: 0.6116\n",
            "Epoch [6/10], Step [300/625], Loss: 0.4740\n",
            "Epoch [6/10], Step [310/625], Loss: 0.5697\n",
            "Epoch [6/10], Step [320/625], Loss: 0.4895\n",
            "Epoch [6/10], Step [330/625], Loss: 0.7705\n",
            "Epoch [6/10], Step [340/625], Loss: 0.6289\n",
            "Epoch [6/10], Step [350/625], Loss: 0.2808\n",
            "Epoch [6/10], Step [360/625], Loss: 0.3544\n",
            "Epoch [6/10], Step [370/625], Loss: 0.6723\n",
            "Epoch [6/10], Step [380/625], Loss: 0.4642\n",
            "Epoch [6/10], Step [390/625], Loss: 0.6225\n",
            "Epoch [6/10], Step [400/625], Loss: 0.5154\n",
            "Epoch [6/10], Step [410/625], Loss: 0.4669\n",
            "Epoch [6/10], Step [420/625], Loss: 0.5942\n",
            "Epoch [6/10], Step [430/625], Loss: 0.7476\n",
            "Epoch [6/10], Step [440/625], Loss: 0.7340\n",
            "Epoch [6/10], Step [450/625], Loss: 0.3944\n",
            "Epoch [6/10], Step [460/625], Loss: 0.5991\n",
            "Epoch [6/10], Step [470/625], Loss: 0.2729\n",
            "Epoch [6/10], Step [480/625], Loss: 0.4537\n",
            "Epoch [6/10], Step [490/625], Loss: 0.5044\n",
            "Epoch [6/10], Step [500/625], Loss: 0.4102\n",
            "Epoch [6/10], Step [510/625], Loss: 0.3244\n",
            "Epoch [6/10], Step [520/625], Loss: 0.6785\n",
            "Epoch [6/10], Step [530/625], Loss: 0.3306\n",
            "Epoch [6/10], Step [540/625], Loss: 0.8616\n",
            "Epoch [6/10], Step [550/625], Loss: 0.3175\n",
            "Epoch [6/10], Step [560/625], Loss: 0.5123\n",
            "Epoch [6/10], Step [570/625], Loss: 0.6087\n",
            "Epoch [6/10], Step [580/625], Loss: 0.3175\n",
            "Epoch [6/10], Step [590/625], Loss: 0.3895\n",
            "Epoch [6/10], Step [600/625], Loss: 0.3211\n",
            "Epoch [6/10], Step [610/625], Loss: 0.3248\n",
            "Epoch [6/10], Step [620/625], Loss: 0.4444\n",
            "Epoch [7/10], Step [10/625], Loss: 0.3222\n",
            "Epoch [7/10], Step [20/625], Loss: 0.3673\n",
            "Epoch [7/10], Step [30/625], Loss: 0.6835\n",
            "Epoch [7/10], Step [40/625], Loss: 0.6949\n",
            "Epoch [7/10], Step [50/625], Loss: 0.3720\n",
            "Epoch [7/10], Step [60/625], Loss: 0.7551\n",
            "Epoch [7/10], Step [70/625], Loss: 0.4769\n",
            "Epoch [7/10], Step [80/625], Loss: 0.3103\n",
            "Epoch [7/10], Step [90/625], Loss: 0.4405\n",
            "Epoch [7/10], Step [100/625], Loss: 0.6025\n",
            "Epoch [7/10], Step [110/625], Loss: 0.6150\n",
            "Epoch [7/10], Step [120/625], Loss: 0.6398\n",
            "Epoch [7/10], Step [130/625], Loss: 0.7236\n",
            "Epoch [7/10], Step [140/625], Loss: 0.6478\n",
            "Epoch [7/10], Step [150/625], Loss: 0.5837\n",
            "Epoch [7/10], Step [160/625], Loss: 1.0519\n",
            "Epoch [7/10], Step [170/625], Loss: 0.3731\n",
            "Epoch [7/10], Step [180/625], Loss: 0.7687\n",
            "Epoch [7/10], Step [190/625], Loss: 0.6898\n",
            "Epoch [7/10], Step [200/625], Loss: 0.4547\n",
            "Epoch [7/10], Step [210/625], Loss: 0.6424\n",
            "Epoch [7/10], Step [220/625], Loss: 0.4126\n",
            "Epoch [7/10], Step [230/625], Loss: 0.7099\n",
            "Epoch [7/10], Step [240/625], Loss: 0.5388\n",
            "Epoch [7/10], Step [250/625], Loss: 0.5892\n",
            "Epoch [7/10], Step [260/625], Loss: 0.5536\n",
            "Epoch [7/10], Step [270/625], Loss: 0.6795\n",
            "Epoch [7/10], Step [280/625], Loss: 0.5101\n",
            "Epoch [7/10], Step [290/625], Loss: 0.6007\n",
            "Epoch [7/10], Step [300/625], Loss: 0.4745\n",
            "Epoch [7/10], Step [310/625], Loss: 0.6476\n",
            "Epoch [7/10], Step [320/625], Loss: 0.4921\n",
            "Epoch [7/10], Step [330/625], Loss: 0.6387\n",
            "Epoch [7/10], Step [340/625], Loss: 0.6124\n",
            "Epoch [7/10], Step [350/625], Loss: 0.3689\n",
            "Epoch [7/10], Step [360/625], Loss: 0.4099\n",
            "Epoch [7/10], Step [370/625], Loss: 0.5286\n",
            "Epoch [7/10], Step [380/625], Loss: 0.6458\n",
            "Epoch [7/10], Step [390/625], Loss: 0.5630\n",
            "Epoch [7/10], Step [400/625], Loss: 0.3847\n",
            "Epoch [7/10], Step [410/625], Loss: 0.5499\n",
            "Epoch [7/10], Step [420/625], Loss: 0.6430\n",
            "Epoch [7/10], Step [430/625], Loss: 0.7306\n",
            "Epoch [7/10], Step [440/625], Loss: 0.7138\n",
            "Epoch [7/10], Step [450/625], Loss: 0.6913\n",
            "Epoch [7/10], Step [460/625], Loss: 0.6755\n",
            "Epoch [7/10], Step [470/625], Loss: 0.3332\n",
            "Epoch [7/10], Step [480/625], Loss: 0.5207\n",
            "Epoch [7/10], Step [490/625], Loss: 0.4358\n",
            "Epoch [7/10], Step [500/625], Loss: 0.4993\n",
            "Epoch [7/10], Step [510/625], Loss: 0.3768\n",
            "Epoch [7/10], Step [520/625], Loss: 0.6699\n",
            "Epoch [7/10], Step [530/625], Loss: 0.4017\n",
            "Epoch [7/10], Step [540/625], Loss: 0.5788\n",
            "Epoch [7/10], Step [550/625], Loss: 0.5805\n",
            "Epoch [7/10], Step [560/625], Loss: 0.4298\n",
            "Epoch [7/10], Step [570/625], Loss: 0.6439\n",
            "Epoch [7/10], Step [580/625], Loss: 0.3728\n",
            "Epoch [7/10], Step [590/625], Loss: 0.4039\n",
            "Epoch [7/10], Step [600/625], Loss: 0.2524\n",
            "Epoch [7/10], Step [610/625], Loss: 0.3192\n",
            "Epoch [7/10], Step [620/625], Loss: 0.3966\n",
            "Epoch [8/10], Step [10/625], Loss: 0.3277\n",
            "Epoch [8/10], Step [20/625], Loss: 0.3342\n",
            "Epoch [8/10], Step [30/625], Loss: 0.6058\n",
            "Epoch [8/10], Step [40/625], Loss: 0.8070\n",
            "Epoch [8/10], Step [50/625], Loss: 0.4820\n",
            "Epoch [8/10], Step [60/625], Loss: 0.6468\n",
            "Epoch [8/10], Step [70/625], Loss: 0.5271\n",
            "Epoch [8/10], Step [80/625], Loss: 0.3734\n",
            "Epoch [8/10], Step [90/625], Loss: 0.4165\n",
            "Epoch [8/10], Step [100/625], Loss: 0.5864\n",
            "Epoch [8/10], Step [110/625], Loss: 0.6062\n",
            "Epoch [8/10], Step [120/625], Loss: 0.5178\n",
            "Epoch [8/10], Step [130/625], Loss: 0.5936\n",
            "Epoch [8/10], Step [140/625], Loss: 0.5069\n",
            "Epoch [8/10], Step [150/625], Loss: 0.5537\n",
            "Epoch [8/10], Step [160/625], Loss: 0.8001\n",
            "Epoch [8/10], Step [170/625], Loss: 0.4068\n",
            "Epoch [8/10], Step [180/625], Loss: 0.6863\n",
            "Epoch [8/10], Step [190/625], Loss: 0.7046\n",
            "Epoch [8/10], Step [200/625], Loss: 0.4382\n",
            "Epoch [8/10], Step [210/625], Loss: 0.6690\n",
            "Epoch [8/10], Step [220/625], Loss: 0.4526\n",
            "Epoch [8/10], Step [230/625], Loss: 0.6723\n",
            "Epoch [8/10], Step [240/625], Loss: 0.5454\n",
            "Epoch [8/10], Step [250/625], Loss: 0.7834\n",
            "Epoch [8/10], Step [260/625], Loss: 0.5821\n",
            "Epoch [8/10], Step [270/625], Loss: 0.7512\n",
            "Epoch [8/10], Step [280/625], Loss: 0.4804\n",
            "Epoch [8/10], Step [290/625], Loss: 0.7472\n",
            "Epoch [8/10], Step [300/625], Loss: 0.4912\n",
            "Epoch [8/10], Step [310/625], Loss: 0.5748\n",
            "Epoch [8/10], Step [320/625], Loss: 0.3922\n",
            "Epoch [8/10], Step [330/625], Loss: 0.6547\n",
            "Epoch [8/10], Step [340/625], Loss: 0.5118\n",
            "Epoch [8/10], Step [350/625], Loss: 0.3142\n",
            "Epoch [8/10], Step [360/625], Loss: 0.4367\n",
            "Epoch [8/10], Step [370/625], Loss: 0.6309\n",
            "Epoch [8/10], Step [380/625], Loss: 0.5832\n",
            "Epoch [8/10], Step [390/625], Loss: 0.5578\n",
            "Epoch [8/10], Step [400/625], Loss: 0.5458\n",
            "Epoch [8/10], Step [410/625], Loss: 0.5277\n",
            "Epoch [8/10], Step [420/625], Loss: 0.6207\n",
            "Epoch [8/10], Step [430/625], Loss: 0.7555\n",
            "Epoch [8/10], Step [440/625], Loss: 0.7216\n",
            "Epoch [8/10], Step [450/625], Loss: 0.5372\n",
            "Epoch [8/10], Step [460/625], Loss: 0.5681\n",
            "Epoch [8/10], Step [470/625], Loss: 0.3287\n",
            "Epoch [8/10], Step [480/625], Loss: 0.4495\n",
            "Epoch [8/10], Step [490/625], Loss: 0.5144\n",
            "Epoch [8/10], Step [500/625], Loss: 0.4598\n",
            "Epoch [8/10], Step [510/625], Loss: 0.4309\n",
            "Epoch [8/10], Step [520/625], Loss: 0.7948\n",
            "Epoch [8/10], Step [530/625], Loss: 0.5358\n",
            "Epoch [8/10], Step [540/625], Loss: 0.7809\n",
            "Epoch [8/10], Step [550/625], Loss: 0.4350\n",
            "Epoch [8/10], Step [560/625], Loss: 0.4113\n",
            "Epoch [8/10], Step [570/625], Loss: 0.7910\n",
            "Epoch [8/10], Step [580/625], Loss: 0.3782\n",
            "Epoch [8/10], Step [590/625], Loss: 0.3840\n",
            "Epoch [8/10], Step [600/625], Loss: 0.2508\n",
            "Epoch [8/10], Step [610/625], Loss: 0.4207\n",
            "Epoch [8/10], Step [620/625], Loss: 0.4701\n",
            "Epoch [9/10], Step [10/625], Loss: 0.3677\n",
            "Epoch [9/10], Step [20/625], Loss: 0.3786\n",
            "Epoch [9/10], Step [30/625], Loss: 0.5196\n",
            "Epoch [9/10], Step [40/625], Loss: 0.8007\n",
            "Epoch [9/10], Step [50/625], Loss: 0.3945\n",
            "Epoch [9/10], Step [60/625], Loss: 0.6881\n",
            "Epoch [9/10], Step [70/625], Loss: 0.4995\n",
            "Epoch [9/10], Step [80/625], Loss: 0.4016\n",
            "Epoch [9/10], Step [90/625], Loss: 0.3793\n",
            "Epoch [9/10], Step [100/625], Loss: 0.5687\n",
            "Epoch [9/10], Step [110/625], Loss: 0.6581\n",
            "Epoch [9/10], Step [120/625], Loss: 0.5113\n",
            "Epoch [9/10], Step [130/625], Loss: 0.6362\n",
            "Epoch [9/10], Step [140/625], Loss: 0.7287\n",
            "Epoch [9/10], Step [150/625], Loss: 0.5357\n",
            "Epoch [9/10], Step [160/625], Loss: 0.8301\n",
            "Epoch [9/10], Step [170/625], Loss: 0.2655\n",
            "Epoch [9/10], Step [180/625], Loss: 0.7436\n",
            "Epoch [9/10], Step [190/625], Loss: 0.6987\n",
            "Epoch [9/10], Step [200/625], Loss: 0.3942\n",
            "Epoch [9/10], Step [210/625], Loss: 0.5871\n",
            "Epoch [9/10], Step [220/625], Loss: 0.5540\n",
            "Epoch [9/10], Step [230/625], Loss: 0.6649\n",
            "Epoch [9/10], Step [240/625], Loss: 0.5368\n",
            "Epoch [9/10], Step [250/625], Loss: 0.7237\n",
            "Epoch [9/10], Step [260/625], Loss: 0.6429\n",
            "Epoch [9/10], Step [270/625], Loss: 0.6915\n",
            "Epoch [9/10], Step [280/625], Loss: 0.5050\n",
            "Epoch [9/10], Step [290/625], Loss: 0.5749\n",
            "Epoch [9/10], Step [300/625], Loss: 0.6047\n",
            "Epoch [9/10], Step [310/625], Loss: 0.7282\n",
            "Epoch [9/10], Step [320/625], Loss: 0.4379\n",
            "Epoch [9/10], Step [330/625], Loss: 0.7175\n",
            "Epoch [9/10], Step [340/625], Loss: 0.5240\n",
            "Epoch [9/10], Step [350/625], Loss: 0.2604\n",
            "Epoch [9/10], Step [360/625], Loss: 0.3641\n",
            "Epoch [9/10], Step [370/625], Loss: 0.6705\n",
            "Epoch [9/10], Step [380/625], Loss: 0.5642\n",
            "Epoch [9/10], Step [390/625], Loss: 0.5915\n",
            "Epoch [9/10], Step [400/625], Loss: 0.5371\n",
            "Epoch [9/10], Step [410/625], Loss: 0.5155\n",
            "Epoch [9/10], Step [420/625], Loss: 0.5147\n",
            "Epoch [9/10], Step [430/625], Loss: 0.8215\n",
            "Epoch [9/10], Step [440/625], Loss: 0.6512\n",
            "Epoch [9/10], Step [450/625], Loss: 0.4565\n",
            "Epoch [9/10], Step [460/625], Loss: 0.6124\n",
            "Epoch [9/10], Step [470/625], Loss: 0.3441\n",
            "Epoch [9/10], Step [480/625], Loss: 0.5223\n",
            "Epoch [9/10], Step [490/625], Loss: 0.4716\n",
            "Epoch [9/10], Step [500/625], Loss: 0.4420\n",
            "Epoch [9/10], Step [510/625], Loss: 0.3901\n",
            "Epoch [9/10], Step [520/625], Loss: 0.7482\n",
            "Epoch [9/10], Step [530/625], Loss: 0.4220\n",
            "Epoch [9/10], Step [540/625], Loss: 0.6751\n",
            "Epoch [9/10], Step [550/625], Loss: 0.4829\n",
            "Epoch [9/10], Step [560/625], Loss: 0.4777\n",
            "Epoch [9/10], Step [570/625], Loss: 0.6811\n",
            "Epoch [9/10], Step [580/625], Loss: 0.4835\n",
            "Epoch [9/10], Step [590/625], Loss: 0.3777\n",
            "Epoch [9/10], Step [600/625], Loss: 0.2404\n",
            "Epoch [9/10], Step [610/625], Loss: 0.3850\n",
            "Epoch [9/10], Step [620/625], Loss: 0.4703\n",
            "Epoch [10/10], Step [10/625], Loss: 0.3679\n",
            "Epoch [10/10], Step [20/625], Loss: 0.3846\n",
            "Epoch [10/10], Step [30/625], Loss: 0.4786\n",
            "Epoch [10/10], Step [40/625], Loss: 0.6549\n",
            "Epoch [10/10], Step [50/625], Loss: 0.4760\n",
            "Epoch [10/10], Step [60/625], Loss: 0.7188\n",
            "Epoch [10/10], Step [70/625], Loss: 0.5074\n",
            "Epoch [10/10], Step [80/625], Loss: 0.4748\n",
            "Epoch [10/10], Step [90/625], Loss: 0.3704\n",
            "Epoch [10/10], Step [100/625], Loss: 0.4890\n",
            "Epoch [10/10], Step [110/625], Loss: 0.6194\n",
            "Epoch [10/10], Step [120/625], Loss: 0.5242\n",
            "Epoch [10/10], Step [130/625], Loss: 0.6616\n",
            "Epoch [10/10], Step [140/625], Loss: 0.6809\n",
            "Epoch [10/10], Step [150/625], Loss: 0.5727\n",
            "Epoch [10/10], Step [160/625], Loss: 0.6109\n",
            "Epoch [10/10], Step [170/625], Loss: 0.2870\n",
            "Epoch [10/10], Step [180/625], Loss: 0.6778\n",
            "Epoch [10/10], Step [190/625], Loss: 0.8919\n",
            "Epoch [10/10], Step [200/625], Loss: 0.5023\n",
            "Epoch [10/10], Step [210/625], Loss: 0.5945\n",
            "Epoch [10/10], Step [220/625], Loss: 0.6188\n",
            "Epoch [10/10], Step [230/625], Loss: 0.7699\n",
            "Epoch [10/10], Step [240/625], Loss: 0.5878\n",
            "Epoch [10/10], Step [250/625], Loss: 0.7592\n",
            "Epoch [10/10], Step [260/625], Loss: 0.6250\n",
            "Epoch [10/10], Step [270/625], Loss: 0.6686\n",
            "Epoch [10/10], Step [280/625], Loss: 0.5098\n",
            "Epoch [10/10], Step [290/625], Loss: 0.6871\n",
            "Epoch [10/10], Step [300/625], Loss: 0.5840\n",
            "Epoch [10/10], Step [310/625], Loss: 0.6724\n",
            "Epoch [10/10], Step [320/625], Loss: 0.4880\n",
            "Epoch [10/10], Step [330/625], Loss: 0.6083\n",
            "Epoch [10/10], Step [340/625], Loss: 0.5577\n",
            "Epoch [10/10], Step [350/625], Loss: 0.2156\n",
            "Epoch [10/10], Step [360/625], Loss: 0.3461\n",
            "Epoch [10/10], Step [370/625], Loss: 0.5734\n",
            "Epoch [10/10], Step [380/625], Loss: 0.6226\n",
            "Epoch [10/10], Step [390/625], Loss: 0.5276\n",
            "Epoch [10/10], Step [400/625], Loss: 0.4876\n",
            "Epoch [10/10], Step [410/625], Loss: 0.4403\n",
            "Epoch [10/10], Step [420/625], Loss: 0.5544\n",
            "Epoch [10/10], Step [430/625], Loss: 0.7076\n",
            "Epoch [10/10], Step [440/625], Loss: 0.7361\n",
            "Epoch [10/10], Step [450/625], Loss: 0.4609\n",
            "Epoch [10/10], Step [460/625], Loss: 0.5424\n",
            "Epoch [10/10], Step [470/625], Loss: 0.2608\n",
            "Epoch [10/10], Step [480/625], Loss: 0.3896\n",
            "Epoch [10/10], Step [490/625], Loss: 0.4414\n",
            "Epoch [10/10], Step [500/625], Loss: 0.5232\n",
            "Epoch [10/10], Step [510/625], Loss: 0.3735\n",
            "Epoch [10/10], Step [520/625], Loss: 0.6837\n",
            "Epoch [10/10], Step [530/625], Loss: 0.5006\n",
            "Epoch [10/10], Step [540/625], Loss: 0.6237\n",
            "Epoch [10/10], Step [550/625], Loss: 0.4884\n",
            "Epoch [10/10], Step [560/625], Loss: 0.4408\n",
            "Epoch [10/10], Step [570/625], Loss: 0.5642\n",
            "Epoch [10/10], Step [580/625], Loss: 0.3258\n",
            "Epoch [10/10], Step [590/625], Loss: 0.3565\n",
            "Epoch [10/10], Step [600/625], Loss: 0.2731\n",
            "Epoch [10/10], Step [610/625], Loss: 0.3677\n",
            "Epoch [10/10], Step [620/625], Loss: 0.4613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpUiuADXSOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a9282c-52e7-4dc9-a716-a3de071b55a0"
      },
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.reshape(-1, 256*256*3).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 1000 test images: {acc} %')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 1000 test images: 70.6 %\n"
          ]
        }
      ]
    }
  ]
}